{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: CNN Dataset Generation\n",
    "\n",
    "## ğŸ¯ Overview\n",
    "\n",
    "This notebook demonstrates the CNN dataset generation process. We combine ESM2 embeddings with template-derived features to create multi-channel input tensors for binary contact prediction.\n",
    "\n",
    "## ğŸ“‹ Learning Objectives\n",
    "\n",
    "- Understand multi-channel input tensor creation\n",
    "- Integrate ESM2 embeddings with template features\n",
    "- Generate synchronized CNN training datasets\n",
    "- Validate dataset quality and consistency\n",
    "\n",
    "## ğŸ”¬ Key Concepts\n",
    "\n",
    "**Multi-channel Input**: 68-channel tensors combining sequence and structural features.\n",
    "\n",
    "**ESM2 Embeddings**: 64 channels of learned protein sequence representations.\n",
    "\n",
    "**Template Features**: 4 channels of structural information from homologous templates.\n",
    "\n",
    "**Binary Targets**: 0/1 contact maps for supervised learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src.esm2_contact.dataset.processing import CNNDataGenerator\n",
    "from src.esm2_contact.training.dataset import Tiny10Dataset\n",
    "\n",
    "print(\"âœ… Dependencies imported successfully\")\n",
    "print(f\"ğŸ“ Project root: {project_root}\")\n",
    "print(f\"ğŸ Python version: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Prerequisites Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prerequisites():\n",
    "    \"\"\"\n",
    "    Check that all prerequisite data files exist.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” Checking Prerequisites\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    data_dir = project_root / \"data\" / \"tiny_10\"\n",
    "    \n",
    "    required_files = {\n",
    "        \"ground_truth.h5\": \"Binary contact maps from Step 1\",\n",
    "        \"sequences.json\": \"Protein sequences\", \n",
    "        \"homology_results.json\": \"Template search results from Step 2\"\n",
    "    }\n",
    "    \n",
    "    all_exist = True\n",
    "    \n",
    "    for filename, description in required_files.items():\n",
    "        filepath = data_dir / filename\n",
    "        exists = filepath.exists()\n",
    "        status = \"âœ…\" if exists else \"âŒ\"\n",
    "        print(f\"   {status} {filename}: {description}\")\n",
    "        \n",
    "        if exists:\n",
    "            size_mb = filepath.stat().st_size / (1024 * 1024)\n",
    "            print(f\"      Size: {size_mb:.1f} MB\")\n",
    "        \n",
    "        all_exist = all_exist and exists\n",
    "    \n",
    "    # Check for ESM2 embeddings\n",
    "    embeddings_dir = project_root / \"data\" / \"esm2_embeddings\"\n",
    "    embeddings_exist = embeddings_dir.exists() and len(list(embeddings_dir.glob(\"*.npy\"))) > 0\n",
    "    print(f\"   {'âœ…' if embeddings_exist else 'âŒ'} ESM2 embeddings: Language model features\")\n",
    "    \n",
    "    if embeddings_exist:\n",
    "        embedding_files = list(embeddings_dir.glob(\"*.npy\"))\n",
    "        print(f\"      Found {len(embedding_files)} embedding files\")\n",
    "    \n",
    "    all_exist = all_exist and embeddings_exist\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Prerequisite Status: {'âœ… Complete' if all_exist else 'âŒ Missing files'}\")\n",
    "    \n",
    "    if not all_exist:\n",
    "        print(\"\\nâš ï¸  Missing prerequisites:\")\n",
    "        if not (data_dir / \"ground_truth.h5\").exists():\n",
    "            print(\"   â€¢ Run Step 1: uv run python scripts/create_10_ground_truth_fast.py\")\n",
    "        if not (data_dir / \"homology_results.json\").exists():\n",
    "            print(\"   â€¢ Run Step 2: uv run python scripts/run_10_homology.py\")\n",
    "        if not embeddings_exist:\n",
    "            print(\"   â€¢ Generate ESM2 embeddings using compute_esm2_embeddings.py\")\n",
    "    \n",
    "    return all_exist\n",
    "\n",
    "# Check prerequisites\n",
    "prereqs_ok = check_prerequisites()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Understanding Multi-channel Input Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_input_architecture():\n",
    "    \"\"\"\n",
    "    Explain the 68-channel input architecture for the CNN.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§  CNN Input Architecture Explanation\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    print(\"\\nğŸ“Š Multi-channel Input Structure:\")\n",
    "    print(\"   Total channels: 68\")\n",
    "    print(\"   Input shape: (68, L, L) where L = sequence length\")\n",
    "    \n",
    "    print(\"\\nğŸ”¬ Channel Breakdown:\")\n",
    "    \n",
    "    # Template channels (4)\n",
    "    template_channels = [\n",
    "        \"Template Distance Map\",\n",
    "        \"Template Contact Map\", \n",
    "        \"Template Coverage Map\",\n",
    "        \"Template Confidence Map\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n   ğŸ—ï¸  Template Channels (4):\")\n",
    "    for i, channel in enumerate(template_channels, 1):\n",
    "        print(f\"      Channel {i}: {channel}\")\n",
    "    \n",
    "    # ESM2 embedding channels (64)\n",
    "    print(\"\\n   ğŸ§¬ ESM2 Embedding Channels (64):\")\n",
    "    print(\"      Channels 5-68: Outer product of ESM2 embeddings\")\n",
    "    print(\"      Captures sequence-residue relationships\")\n",
    "    print(\"      Provides evolutionary and structural context\")\n",
    "    \n",
    "    print(\"\\nğŸ”„ Data Integration Process:\")\n",
    "    print(\"   1. Load ESM2 embeddings (L Ã— 64 matrix)\")\n",
    "    print(\"   2. Compute outer product: (L Ã— 64) Ã— (64 Ã— L) = L Ã— L Ã— 64\")\n",
    "    print(\"   3. Process template features into 4 L Ã— L matrices\")\n",
    "    print(\"   4. Stack all channels: 64 + 4 = 68 channels total\")\n",
    "    print(\"   5. Synchronize with ground truth contact maps\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ Benefits of Multi-channel Approach:\")\n",
    "    print(\"   â€¢ Combines sequence information (ESM2) with structural templates\")\n",
    "    print(\"   â€¢ Provides rich feature representation for CNN learning\")\n",
    "    print(\"   â€¢ Enables model to leverage both homology and language model cues\")\n",
    "    print(\"   â€¢ Supports Strategy 1: Homology-Assisted CNN approach\")\n",
    "    \n",
    "    # Create a visual representation\n",
    "    print(\"\\nğŸ“ˆ Visual Representation:\")\n",
    "    print(\"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"   â”‚           Input Tensor (68, L, L)      â”‚\")\n",
    "    print(\"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "    print(\"   â”‚ Template Channels 1-4 (4 Ã— L Ã— L)     â”‚\")\n",
    "    print(\"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "    print(\"   â”‚ ESM2 Outer Product (64 Ã— L Ã— L)       â”‚\")\n",
    "    print(\"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    print(\"                    â†“\")\n",
    "    print(\"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"   â”‚            Binary CNN Model           â”‚\")\n",
    "    print(\"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "                    â†“\n",
    "    print(\"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"   â”‚         Contact Map (L Ã— L)           â”‚\")\n",
    "    print(\"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "# Explain architecture\n",
    "explain_input_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Loading and Analyzing Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_input_data():\n",
    "    \"\"\"\n",
    "    Load and analyze all input data components for CNN dataset generation.\n",
    "    \"\"\"\n",
    "    if not prereqs_ok:\n",
    "        print(\"âŒ Prerequisites not met\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ“‚ Loading Input Data Components\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    data_dir = project_root / \"data\" / \"tiny_10\"\n",
    "    \n",
    "    # Load ground truth data\n",
    "    print(\"\\nğŸ¯ Loading Ground Truth Data...\")\n",
    "    with h5py.File(data_dir / \"ground_truth.h5\", 'r') as f:\n",
    "        ground_truth = {}\n",
    "        for protein_id in f.keys():\n",
    "            ground_truth[protein_id] = {\n",
    "                'contact_map': f[protein_id]['contact_map'][:],\n",
    "                'sequence': f[protein_id]['sequence'][:].decode('utf-8')\n",
    "            }\n",
    "    \n",
    "    print(f\"   Loaded {len(ground_truth)} proteins with ground truth\")\n",
    "    \n",
    "    # Load homology results\n",
    "    print(\"\\nğŸ” Loading Homology Results...\")\n",
    "    with open(data_dir / \"homology_results.json\", 'r') as f:\n",
    "        homology_results = json.load(f)\n",
    "    \n",
    "    query_results = homology_results.get('results', {})\n",
    "    print(f\"   Loaded homology results for {len(query_results)} queries\")\n",
    "    \n",
    "    # Check ESM2 embeddings\n",
    "    print(\"\\nğŸ§  Checking ESM2 Embeddings...\")\n",
    "    embeddings_dir = project_root / \"data\" / \"esm2_embeddings\"\n",
    "    embedding_files = list(embeddings_dir.glob(\"*.npy\"))\n",
    "    \n",
    "    print(f\"   Found {len(embedding_files)} embedding files\")\n",
    "    \n",
    "    # Analyze data consistency\n",
    "    print(\"\\nğŸ“Š Data Consistency Analysis:\")\n",
    "    \n",
    "    common_proteins = set(ground_truth.keys()) & set(query_results.keys())\n",
    "    print(f\"   Proteins with ground truth: {len(ground_truth)}\")\n",
    "    print(f\"   Proteins with homology results: {len(query_results)}\")\n",
    "    print(f\"   Common proteins: {len(common_proteins)}\")\n",
    "    \n",
    "    # Check protein with templates\n",
    "    proteins_with_templates = [pid for pid, result in query_results.items() \n",
    "                               if result.get('templates')]\n",
    "    print(f\"   Proteins with templates: {len(proteins_with_templates)}\")\n",
    "    \n",
    "    # Sample protein analysis\n",
    "    if common_proteins:\n",
    "        sample_protein = list(common_proteins)[0]\n",
    "        print(f\"\\nğŸ“‹ Sample Analysis: {sample_protein}\")\n",
    "        \n",
    "        gt_data = ground_truth[sample_protein]\n",
    "        holo_data = query_results[sample_protein]\n",
    "        \n",
    "        print(f\"   Sequence length: {len(gt_data['sequence'])}\")\n",
    "        print(f\"   Contact map shape: {gt_data['contact_map'].shape}\")\n",
    "        print(f\"   Contact density: {np.mean(gt_data['contact_map']):.4f}\")\n",
    "        print(f\"   Templates found: {len(holo_data.get('templates', []))}\")\n",
    "        \n",
    "        # Check for embedding file\n",
    "        embedding_file = embeddings_dir / f\"{sample_protein}.npy\"\n",
    "        if embedding_file.exists():\n",
    "            embedding = np.load(embedding_file)\n",
    "            print(f\"   ESM2 embedding shape: {embedding.shape}\")\n",
    "        else:\n",
    "            print(f\"   ESM2 embedding: Missing\")\n",
    "    \n",
    "    return {\n",
    "        'ground_truth': ground_truth,\n",
    "        'homology_results': query_results,\n",
    "        'common_proteins': common_proteins,\n",
    "        'proteins_with_templates': proteins_with_templates\n",
    "    }\n",
    "\n",
    "# Load input data\n",
    "input_data = load_and_analyze_input_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ CNN Data Generation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_cnn_data_generation():\n",
    "    \"\"\"\n",
    "    Demonstrate the CNN data generation process.\n",
    "    \"\"\"\n",
    "    if not input_data:\n",
    "        print(\"âŒ No input data available\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ”¬ CNN Data Generation Demonstration\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    # Initialize data generator\n",
    "    data_dir = project_root / \"data\" / \"tiny_10\"\n",
    "    embeddings_dir = project_root / \"data\" / \"esm2_embeddings\"\n",
    "    \n",
    "    print(\"\\nâš™ï¸ Data Generator Configuration:\")\n",
    "    print(f\"   Ground truth file: {data_dir / 'ground_truth.h5'}\")\n",
    "    print(f\"   Homology results: {data_dir / 'homology_results.json'}\")\n",
    "    print(f\"   ESM2 embeddings: {embeddings_dir}\")\n",
    "    print(f\"   Output file: {data_dir / 'cnn_dataset.h5'}\")\n",
    "    \n",
    "    # Sample the generation process for one protein\n",
    "    common_proteins = input_data['common_proteins']\n",
    "    proteins_with_templates = input_data['proteins_with_templates']\n",
    "    \n",
    "    # Find a protein with all required data\n",
    "    sample_protein = None\n",
    "    for protein in common_proteins:\n",
    "        if protein in proteins_with_templates:\n",
    "            embedding_file = embeddings_dir / f\"{protein}.npy\"\n",
    "            if embedding_file.exists():\n",
    "                sample_protein = protein\n",
    "                break\n",
    "    \n",
    "    if not sample_protein:\n",
    "        print(\"âŒ No protein found with all required data\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nğŸ§¬ Processing Sample Protein: {sample_protein}\")\n",
    "    \n",
    "    # Load data for sample protein\n",
    "    gt_data = input_data['ground_truth'][sample_protein]\n",
    "    holo_data = input_data['homology_results'][sample_protein]\n",
    "    embedding = np.load(embeddings_dir / f\"{sample_protein}.npy\")\n",
    "    \n",
    "    sequence = gt_data['sequence']\n",
    "    contact_map = gt_data['contact_map']\n",
    "    templates = holo_data.get('templates', [])\n",
    "    \n",
    "    print(f\"   Sequence length: {len(sequence)}\")\n",
    "    print(f\"   Contact map shape: {contact_map.shape}\")\n",
    "    print(f\"   ESM2 embedding shape: {embedding.shape}\")\n",
    "    print(f\"   Available templates: {len(templates)}\")\n",
    "    \n",
    "    # Demonstrate outer product computation\n",
    "    print(\"\\nğŸ”„ ESM2 Outer Product Computation:\")\n",
    "    print(f\"   Input shape: {embedding.shape}\")\n",
    "    \n",
    "    # Compute outer product (this is what the CNN data generator does)\n",
    "    outer_product = np.outer(embedding, embedding)\n",
    "    print(f\"   Outer product shape: {outer_product.shape}\")\n",
    "    \n",
    "    # Reshape to (64, L, L) format\n",
    "    L = len(sequence)\n",
    "    esm2_channels = outer_product.reshape(64, L, L)\n",
    "    print(f\"   ESM2 channels shape: {esm2_channels.shape}\")\n",
    "    \n",
    "    print(\"\\nğŸ—ï¸ Template Feature Processing:\")\n",
    "    if templates:\n",
    "        best_template = templates[0]\n",
    "        print(f\"   Best template: {best_template.get('template_pdb', 'Unknown')}\")\n",
    "        print(f\"   Template identity: {best_template.get('sequence_identity', 0):.3f}\")\n",
    "        print(f\"   Template coverage: {best_template.get('coverage', 0):.3f}\")\n",
    "        \n",
    "        # Create synthetic template features for demonstration\n",
    "        # (In real process, these are computed from actual template structures)\n",
    "        template_distance_map = np.random.rand(L, L) * 20  # Random distances\n",
    "        template_contact_map = (template_distance_map < 8).astype(float)  # Binary contacts\n",
    "        template_coverage_map = np.ones((L, L)) * best_template.get('coverage', 0.5)\n",
    "        template_confidence_map = np.ones((L, L)) * best_template.get('sequence_identity', 0.5)\n",
    "        \n",
    "        template_channels = np.stack([\n",
    "            template_distance_map,\n",
    "            template_contact_map,\n",
    "            template_coverage_map,\n",
    "            template_confidence_map\n",
    "        ], axis=0)\n",
    "        \n",
    "        print(f\"   Template channels shape: {template_channels.shape}\")\n",
    "    else:\n",
    "        # Create dummy template features\n",
    "        template_channels = np.zeros((4, L, L))\n",
    "        print(f\"   No templates found, using dummy features: {template_channels.shape}\")\n",
    "    \n",
    "    # Combine all channels\n",
    "    print(\"\\nğŸ”— Channel Combination:\")\n",
    "    multi_channel_input = np.concatenate([template_channels, esm2_channels], axis=0)\n",
    "    print(f\"   Final input shape: {multi_channel_input.shape}\")\n",
    "    print(f\"   Expected channels: 68 (4 template + 64 ESM2)\")\n",
    "    \n",
    "    # Validate channel dimensions\n",
    "    assert multi_channel_input.shape[0] == 68, f\"Expected 68 channels, got {multi_channel_input.shape[0]}\"\n",
    "    assert multi_channel_input.shape[1] == multi_channel_input.shape[2] == L, \"Matrix should be square\"\n",
    "    \n",
    "    print(\"   âœ… Channel dimensions validated\")\n",
    "    \n",
    "    # Data quality checks\n",
    "    print(\"\\nğŸ” Data Quality Checks:\")\n",
    "    print(f\"   Input contains NaN: {np.isnan(multi_channel_input).any()}\")\n",
    "    print(f\"   Input contains Inf: {np.isinf(multi_channel_input).any()}\")\n",
    "    print(f\"   Input range: [{multi_channel_input.min():.4f}, {multi_channel_input.max():.4f}]\")\n",
    "    print(f\"   Contact map density: {np.mean(contact_map):.4f}\")\n",
    "    print(f\"   Contact map range: [{contact_map.min():.4f}, {contact_map.max():.4f}]\")\n",
    "    \n",
    "    return {\n",
    "        'sample_protein': sample_protein,\n",
    "        'multi_channel_input': multi_channel_input,\n",
    "        'contact_map': contact_map,\n",
    "        'sequence': sequence\n",
    "    }\n",
    "\n",
    "# Demonstrate CNN data generation\n",
    "cnn_demo = demonstrate_cnn_data_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Multi-channel Input Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multi_channel_input():\n",
    "    \"\"\"\n",
    "    Visualize the multi-channel input structure.\n",
    "    \"\"\"\n",
    "    if not cnn_demo:\n",
    "        print(\"âŒ No CNN demo data to visualize\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ“Š Multi-channel Input Visualization\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    multi_channel_input = cnn_demo['multi_channel_input']\n",
    "    contact_map = cnn_demo['contact_map']\n",
    "    sequence = cnn_demo['sequence']\n",
    "    sample_protein = cnn_demo['sample_protein']\n",
    "    \n",
    "    L = len(sequence)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    \n",
    "    # Template channels (4) - show first 3\n",
    "    for i in range(3):\n",
    "        im = axes[0, i].imshow(multi_channel_input[i], cmap='viridis', origin='lower')\n",
    "        axes[0, i].set_title(f'Template Channel {i+1}')\n",
    "        axes[0, i].set_xlabel('Residue Index')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel('Residue Index')\n",
    "        plt.colorbar(im, ax=axes[0, i], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # ESM2 channels - show sample channels\n",
    "    esm2_channel_indices = [4, 10, 20]  # Sample from ESM2 channels\n",
    "    for i, channel_idx in enumerate(esm2_channel_indices):\n",
    "        im = axes[1, i].imshow(multi_channel_input[channel_idx], cmap='RdBu_r', origin='lower')\n",
    "        axes[1, i].set_title(f'ESM2 Channel {channel_idx+1}')\n",
    "        axes[1, i].set_xlabel('Residue Index')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel('Residue Index')\n",
    "        plt.colorbar(im, ax=axes[1, i], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Target and statistics\n",
    "    # Ground truth contact map\n",
    "    im = axes[2, 0].imshow(contact_map, cmap='binary', origin='lower')\n",
    "    axes[2, 0].set_title('Ground Truth Contact Map')\n",
    "    axes[2, 0].set_xlabel('Residue Index')\n",
    "    axes[2, 0].set_ylabel('Residue Index')\n",
    "    plt.colorbar(im, ax=axes[2, 0], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Channel statistics\n",
    "    channel_means = np.mean(multi_channel_input, axis=(1, 2))\n",
    "    channel_stds = np.std(multi_channel_input, axis=(1, 2))\n",
    "    \n",
    "    axes[2, 1].plot(channel_means, label='Mean', alpha=0.7)\n",
    "    axes[2, 1].axvline(x=4, color='red', linestyle='--', label='ESM2 start')\n",
    "    axes[2, 1].set_xlabel('Channel Index')\n",
    "    axes[2, 1].set_ylabel('Mean Value')\n",
    "    axes[2, 1].set_title('Channel Means')\n",
    "    axes[2, 1].legend()\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2, 2].plot(channel_stds, label='Std Dev', alpha=0.7, color='orange')\n",
    "    axes[2, 2].axvline(x=4, color='red', linestyle='--', label='ESM2 start')\n",
    "    axes[2, 2].set_xlabel('Channel Index')\n",
    "    axes[2, 2].set_ylabel('Standard Deviation')\n",
    "    axes[2, 2].set_title('Channel Standard Deviations')\n",
    "    axes[2, 2].legend()\n",
    "    axes[2, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Multi-channel CNN Input Analysis - {sample_protein} (L={L})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\nğŸ“ˆ Detailed Channel Statistics:\")\n",
    "    print(f\"   Protein: {sample_protein}\")\n",
    "    print(f\"   Sequence length: {L}\")\n",
    "    print(f\"   Input shape: {multi_channel_input.shape}\")\n",
    "    \n",
    "    print(f\"\\n   Template Channels (1-4):\")\n",
    "    for i in range(4):\n",
    "        channel_data = multi_channel_input[i]\n",
    "        print(f\"      Channel {i+1}: mean={np.mean(channel_data):.4f}, \"\n",
    "              f\"std={np.std(channel_data):.4f}, \"\n",
    "              f\"range=[{np.min(channel_data):.4f}, {np.max(channel_data):.4f}]\")\n",
    "    \n",
    "    print(f\"\\n   ESM2 Channels (5-68) - Summary:\")\n",
    "    esm2_data = multi_channel_input[4:]  # Channels 5-68\n",
    "    print(f\"      Mean: {np.mean(esm2_data):.4f}\")\n",
    "    print(f\"      Std: {np.std(esm2_data):.4f}\")\n",
    "    print(f\"      Range: [{np.min(esm2_data):.4f}, {np.max(esm2_data):.4f}]\")\n",
    "    print(f\"      Negative values: {np.sum(esm2_data < 0)} ({np.mean(esm2_data < 0)*100:.1f}%)\")\n",
    "    print(f\"      Positive values: {np.sum(esm2_data > 0)} ({np.mean(esm2_data > 0)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n   Target Contact Map:\")\n",
    "    print(f\"      Density: {np.mean(contact_map):.4f}\")\n",
    "    print(f\"      Total contacts: {np.sum(contact_map)}\")\n",
    "    print(f\"      Contact percentage: {np.mean(contact_map)*100:.2f}%\")\n",
    "\n",
    "# Visualize multi-channel input\n",
    "visualize_multi_channel_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Running CNN Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_dataset_generation():\n",
    "    \"\"\"\n",
    "    Execute the complete CNN dataset generation pipeline.\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Running CNN Dataset Generation Pipeline\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    import subprocess\n",
    "    import os\n",
    "    \n",
    "    # Set environment variables\n",
    "    env = os.environ.copy()\n",
    "    env['PYTHONPATH'] = str(project_root)\n",
    "    \n",
    "    # Run the CNN dataset generation script\n",
    "    script_path = project_root / \"scripts\" / \"generate_10_cnn_dataset.py\"\n",
    "    \n",
    "    if not script_path.exists():\n",
    "        print(f\"âŒ Script not found: {script_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"ğŸ“‚ Script: {script_path}\")\n",
    "    print(f\"ğŸ“ Working directory: {project_root}\")\n",
    "    print(\"â±ï¸  This process may take 5-10 minutes...\")\n",
    "    \n",
    "    try:\n",
    "        # Run the CNN dataset generation script\n",
    "        result = subprocess.run(\n",
    "            [\"uv\", \"run\", \"python\", str(script_path)],\n",
    "            cwd=project_root,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            env=env,\n",
    "            timeout=600  # 10 minutes timeout\n",
    "        )\n",
    "        \n",
    "        print(\"ğŸ“Š Pipeline Output:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.stderr:\n",
    "            print(\"\\nâš ï¸ Warnings/Errors:\")\n",
    "            print(result.stderr)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"\\nâœ… CNN dataset generation completed successfully!\")\n",
    "            \n",
    "            # Verify output files\n",
    "            data_dir = project_root / \"data\" / \"tiny_10\"\n",
    "            cnn_dataset_file = data_dir / \"cnn_dataset.h5\"\n",
    "            \n",
    "            if cnn_dataset_file.exists():\n",
    "                file_size_mb = cnn_dataset_file.stat().st_size / (1024 * 1024)\n",
    "                print(f\"\\nğŸ“ Generated files:\")\n",
    "                print(f\"   âœ… {cnn_dataset_file} ({file_size_mb:.1f} MB)\")\n",
    "                \n",
    "                # Quick validation\n",
    "                with h5py.File(cnn_dataset_file, 'r') as f:\n",
    "                    if 'cnn_data' in f:\n",
    "                        protein_count = len(f['cnn_data'].keys())\n",
    "                        print(f\"   ğŸ“Š Processed {protein_count} proteins\")\n",
    "                        \n",
    "                        # Check first protein\n",
    "                        sample_id = list(f['cnn_data'].keys())[0]\n",
    "                        sample_group = f['cnn_data'][sample_id]\n",
    "                        \n",
    "                        if 'multi_channel_input' in sample_group:\n",
    "                            input_shape = sample_group['multi_channel_input'].shape\n",
    "                            print(f\"   ğŸ“ Sample input shape: {input_shape}\")\n",
    "                            \n",
    "                            if 'consensus_contact_map' in sample_group:\n",
    "                                contact_shape = sample_group['consensus_contact_map'].shape\n",
    "                                print(f\"   ğŸ“ Sample contact shape: {contact_shape}\")\n",
    "                        \n",
    "                        return True\n",
    "                    else:\n",
    "                        print(\"   âŒ Invalid dataset format: missing 'cnn_data' group\")\n",
    "                        return False\n",
    "            else:\n",
    "                print(\"âŒ Expected output file not found\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"âŒ Pipeline failed with return code: {result.returncode}\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"âŒ Pipeline timed out after 10 minutes\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error running pipeline: {e}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to run the CNN dataset generation pipeline\n",
    "# success = run_cnn_dataset_generation()\n",
    "# print(f\"\\nğŸ¯ Pipeline {'succeeded' if success else 'failed'}!\")\n",
    "\n",
    "print(\"ğŸ“ CNN dataset generation pipeline ready.\")\n",
    "print(\"ğŸ’¡ Uncomment the function call above to execute the generation.\")\n",
    "print(\"âš ï¸  This will process all 10 proteins and may take 5-10 minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Analyze Generated CNN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_generated_cnn_dataset():\n",
    "    \"\"\"\n",
    "    Analyze the generated CNN dataset for quality and consistency.\n",
    "    \"\"\"\n",
    "    data_dir = project_root / \"data\" / \"tiny_10\"\n",
    "    cnn_dataset_file = data_dir / \"cnn_dataset.h5\"\n",
    "    \n",
    "    if not cnn_dataset_file.exists():\n",
    "        print(\"âŒ No CNN dataset found. Run the generation pipeline first.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ“Š Analyzing Generated CNN Dataset\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    with h5py.File(cnn_dataset_file, 'r') as f:\n",
    "        if 'cnn_data' not in f:\n",
    "            print(\"âŒ Invalid dataset format: missing 'cnn_data' group\")\n",
    "            return None\n",
    "        \n",
    "        cnn_data = f['cnn_data']\n",
    "        protein_ids = list(cnn_data.keys())\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ Dataset Overview:\")\n",
    "        print(f\"   Total proteins: {len(protein_ids)}\")\n",
    "        print(f\"   File size: {cnn_dataset_file.stat().st_size / (1024*1024):.1f} MB\")\n",
    "        \n",
    "        # Analyze each protein\n",
    "        all_lengths = []\n",
    "        all_densities = []\n",
    "        all_input_shapes = []\n",
    "        all_contact_shapes = []\n",
    "        \n",
    "        print(f\"\\nğŸ” Protein-wise Analysis:\")\n",
    "        \n",
    "        for i, protein_id in enumerate(protein_ids):\n",
    "            protein_group = cnn_data[protein_id]\n",
    "            \n",
    "            # Extract data\n",
    "            multi_channel_input = protein_group['multi_channel_input'][:]\n",
    "            contact_map = protein_group['consensus_contact_map'][:]\n",
    "            seq_len = protein_group.attrs.get('sequence_length', contact_map.shape[0])\n",
    "            \n",
    "            # Validate shapes\n",
    "            input_shape = multi_channel_input.shape\n",
    "            contact_shape = contact_map.shape\n",
    "            \n",
    "            # Calculate statistics\n",
    "            density = np.mean(contact_map)\n",
    "            \n",
    "            all_lengths.append(seq_len)\n",
    "            all_densities.append(density)\n",
    "            all_input_shapes.append(input_shape)\n",
    "            all_contact_shapes.append(contact_shape)\n",
    "            \n",
    "            print(f\"   {i+1:2d}. {protein_id}:\")\n",
    "            print(f\"       Length: {seq_len}\")\n",
    "            print(f\"       Input shape: {input_shape}\")\n",
    "            print(f\"       Contact shape: {contact_shape}\")\n",
    "            print(f\"       Contact density: {density:.4f}\")\n",
    "            \n",
    "            # Data quality checks\n",
    "            has_nan = np.isnan(multi_channel_input).any()\n",
    "            has_inf = np.isinf(multi_channel_input).any()\n",
    "            binary_contacts = np.all(np.isin(contact_map, [0.0, 1.0]))\n",
    "            \n",
    "            quality_issues = []\n",
    "            if has_nan:\n",
    "                quality_issues.append(\"NaN values\")\n",
    "            if has_inf:\n",
    "                quality_issues.append(\"Inf values\")\n",
    "            if not binary_contacts:\n",
    "                quality_issues.append(\"Non-binary contacts\")\n",
    "            if input_shape[0] != 68:\n",
    "                quality_issues.append(f\"Wrong channel count: {input_shape[0]}\")\n",
    "            \n",
    "            if quality_issues:\n",
    "                print(f\"       âš ï¸  Issues: {', '.join(quality_issues)}\")\n",
    "            else:\n",
    "                print(f\"       âœ… Quality: OK\")\n",
    "            print()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"ğŸ“ˆ Dataset Statistics:\")\n",
    "        print(f\"   Length range: {min(all_lengths)} - {max(all_lengths)} residues\")\n",
    "        print(f\"   Average length: {np.mean(all_lengths):.1f} residues\")\n",
    "        print(f\"   Density range: {min(all_densities):.4f} - {max(all_densities):.4f}\")\n",
    "        print(f\"   Average density: {np.mean(all_densities):.4f}\")\n",
    "        print(f\"   Total contacts: {sum(np.sum(cnn_data[pid]['consensus_contact_map'][:]) for pid in protein_ids):,}\")\n",
    "        \n",
    "        # Channel statistics\n",
    "        print(f\"\\nğŸ§  Channel Analysis:\")\n",
    "        sample_input = cnn_data[protein_ids[0]]['multi_channel_input'][:]\n",
    "        \n",
    "        template_channels = sample_input[:4]  # First 4 channels\n",
    "        esm2_channels = sample_input[4:]      # Remaining 64 channels\n",
    "        \n",
    "        print(f\"   Template channels (1-4):\")\n",
    "        for i in range(4):\n",
    "            channel_data = template_channels[i]\n",
    "            print(f\"      Channel {i+1}: mean={np.mean(channel_data):.4f}, \"\n",
    "                  f\"std={np.std(channel_data):.4f}\")\n",
    "        \n",
    "        print(f\"   ESM2 channels (5-68):\")\n",
    "        print(f\"      Mean: {np.mean(esm2_channels):.4f}\")\n",
    "        print(f\"      Std: {np.std(esm2_channels):.4f}\")\n",
    "        print(f\"      Range: [{np.min(esm2_channels):.4f}, {np.max(esm2_channels):.4f}]\")\n",
    "        \n",
    "        # Cross-dataset consistency\n",
    "        input_shapes_consistent = all(shape == all_input_shapes[0] for shape in all_input_shapes)\n",
    "        print(f\"\\nğŸ” Consistency Checks:\")\n",
    "        print(f\"   Input shapes consistent: {'âœ…' if input_shapes_consistent else 'âŒ'}\")\n",
    "        print(f\"   All proteins have 68 channels: {'âœ…' if all(s[0] == 68 for s in all_input_shapes) else 'âŒ'}\")\n",
    "        print(f\"   Contact maps are binary: {'âœ…' if all(np.all(np.isin(cnn_data[pid]['consensus_contact_map'][:], [0.0, 1.0])) for pid in protein_ids) else 'âŒ'}\")\n",
    "        \n",
    "        return {\n",
    "            'protein_count': len(protein_ids),\n",
    "            'protein_ids': protein_ids,\n",
    "            'lengths': all_lengths,\n",
    "            'densities': all_densities,\n",
    "            'input_shapes': all_input_shapes,\n",
    "            'contact_shapes': all_contact_shapes\n",
    "        }\n",
    "\n",
    "# Analyze generated dataset\n",
    "dataset_analysis = analyze_generated_cnn_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cnn_dataset_generation():\n",
    "    \"\"\"\n",
    "    Summarize the CNN dataset generation process and results.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“‹ CNN Dataset Generation Summary\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    print(\"\\nğŸ¯ Process Overview:\")\n",
    "    print(\"   1. Load ground truth contact maps (Step 1)\")\n",
    "    print(\"   2. Load homology search results (Step 2)\")\n",
    "    print(\"   3. Load ESM2 embeddings (language model features)\")\n",
    "    print(\"   4. Process template structures into feature maps\")\n",
    "    print(\"   5. Compute ESM2 outer product (64 channels)\")\n",
    "    print(\"   6. Combine into 68-channel input tensors\")\n",
    "    print(\"   7. Synchronize with binary contact map targets\")\n",
    "    \n",
    "    print(\"\\nğŸ”¬ Key Technical Details:\")\n",
    "    print(\"   â€¢ Input format: (68, L, L) tensors per protein\")\n",
    "    print(\"   â€¢ Template channels: 4 (distance, contact, coverage, confidence)\")\n",
    "    print(\"   â€¢ ESM2 channels: 64 (outer product of embeddings)\")\n",
    "    print(\"   â€¢ Target format: Binary LÃ—L contact maps (0/1 values)\")\n",
    "    print(\"   â€¢ Storage: HDF5 format with metadata\")\n",
    "    \n",
    "    if dataset_analysis:\n",
    "        print(f\"\\nğŸ“Š Generated Dataset Statistics:\")\n",
    "        print(f\"   â€¢ Total proteins: {dataset_analysis['protein_count']}\")\n",
    "        print(f\"   â€¢ Length range: {min(dataset_analysis['lengths'])} - {max(dataset_analysis['lengths'])} residues\")\n",
    "        print(f\"   â€¢ Average density: {np.mean(dataset_analysis['densities']):.4f}\")\n",
    "        print(f\"   â€¢ Input channels: 68 (4 template + 64 ESM2)\")\n",
    "        print(f\"   â€¢ Storage format: HDF5 with synchronized data\")\n",
    "        \n",
    "        # Quality assessment\n",
    "        high_density_proteins = sum(1 for d in dataset_analysis['densities'] if d > 0.02)\n",
    "        low_density_proteins = sum(1 for d in dataset_analysis['densities'] if d < 0.005)\n",
    "        \n",
    "        print(f\"\\nğŸ† Dataset Quality Assessment:\")\n",
    "        print(f\"   â€¢ High density proteins (>2%): {high_density_proteins}\")\n",
    "        print(f\"   â€¢ Low density proteins (<0.5%): {low_density_proteins}\")\n",
    "        print(f\"   â€¢ Average density: {np.mean(dataset_analysis['densities']):.4f}\")\n",
    "        print(f\"   â€¢ Density variance: {np.var(dataset_analysis['densities']):.6f}\")\n",
    "    \n",
    "    print(\"\\nâœ… Quality Assurance Features:\")\n",
    "    print(\"   â€¢ NaN and Inf value detection\")\n",
    "    print(\"   â€¢ Binary contact map validation\")\n",
    "    print(\"   â€¢ Shape consistency checking\")\n",
    "    print(\"   â€¢ Channel dimension verification\")\n",
    "    print(\"   â€¢ Data synchronization validation\")\n",
    "    \n",
    "    print(\"\\nğŸ”„ Integration with Strategy 1:\")\n",
    "    print(\"   â€¢ Implements Homology-Assisted CNN approach\")\n",
    "    print(\"   â€¢ Combines ESM2 language model features with template data\")\n",
    "    print(\"   â€¢ Provides rich multi-modal input for deep learning\")\n",
    "    print(\"   â€¢ Supports binary contact prediction as required\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ Dataset Features:\")\n",
    "    print(\"   â€¢ Memory-efficient HDF5 storage\")\n",
    "    print(\"   â€¢ Variable protein length support\")\n",
    "    print(\"   â€¢ Comprehensive metadata inclusion\")\n",
    "    print(\"   â€¢ Ready-to-use for PyTorch DataLoader\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ Ready for Next Step:\")\n",
    "    print(\"   CNN dataset is ready for binary CNN training (Step 4)\")\n",
    "    print(\"   All 68 channels properly formatted and validated\")\n",
    "    print(\"   Binary contact maps synchronized with inputs\")\n",
    "\n",
    "# Run summary\n",
    "summarize_cnn_dataset_generation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}