#!/usr/bin/env python3
"""
CNN Training Script for Protein Contact Prediction

This script provides a command-line interface to the complete training framework
in src/esm2_contact/training/. It offers flexible hyperparameter experimentation
without requiring dataset regeneration.

Key Features:
- Lightweight wrapper leveraging complete training framework
- Flexible train/val/test splitting with reproducible results
- Comprehensive hyperparameter configuration
- Automatic experiment organization and result tracking
- Support for any CNN dataset generated by generate_cnn_dataset_from_pdb.py

Usage Examples:
    # Basic training with default settings
    uv run python scripts/train_cnn.py --dataset-path data/my_dataset/cnn_dataset.h5

    # Custom model architecture
    uv run python scripts/train_cnn.py --dataset-path data/my_dataset/cnn_dataset.h5 \
        --base-channels 64 --dropout-rate 0.2

    # Different data splitting
    uv run python scripts/train_cnn.py --dataset-path data/my_dataset/cnn_dataset.h5 \
        --train-ratio 0.9 --val-ratio 0.05 --test-ratio 0.05

    # Extended training
    uv run python scripts/train_cnn.py --dataset-path data/my_dataset/cnn_dataset.h5 \
        --epochs 100 --learning-rate 5e-4 --weight-decay 1e-4
"""

import os
import sys
import time
import argparse
import warnings
from pathlib import Path
from typing import Dict, Any

import torch
import numpy as np
import json

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import the COMPLETE training framework (no code duplication!)
from src.esm2_contact.training import (
    BinaryContactCNN,      # Model architecture
    Tiny10Dataset,         # Dataset loader (works with any size dataset)
    CNNTrainer,            # Training logic and optimization
    create_data_splits,    # Train/val/test splitting
    collate_fn            # Data batching
)


def parse_arguments() -> argparse.Namespace:
    """Parse command line arguments for training configuration."""
    parser = argparse.ArgumentParser(
        description="Train CNN model for protein contact prediction",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --dataset-path data/cnn_dataset.h5
  %(prog)s --dataset-path data/cnn_dataset.h5 --experiment-name large_model --base-channels 64
  %(prog)s --dataset-path data/cnn_dataset.h5 --train-ratio 0.9 --val-ratio 0.05 --test-ratio 0.05
        """
    )

    # Required arguments
    parser.add_argument(
        '--dataset-path',
        type=str,
        required=True,
        help='Path to CNN dataset HDF5 file generated by generate_cnn_dataset_from_pdb.py'
    )

    parser.add_argument(
        '--experiment-name',
        type=str,
        default='cnn_experiment',
        help='Name for this experiment (used for output directory)'
    )

    # Data splitting arguments
    parser.add_argument(
        '--train-ratio',
        type=float,
        default=0.8,
        help='Training set ratio (default: 0.8)'
    )

    parser.add_argument(
        '--val-ratio',
        type=float,
        default=0.1,
        help='Validation set ratio (default: 0.1)'
    )

    parser.add_argument(
        '--test-ratio',
        type=float,
        default=0.1,
        help='Test set ratio (default: 0.1)'
    )

    parser.add_argument(
        '--random-seed',
        type=int,
        default=42,
        help='Random seed for reproducible data splits (default: 42)'
    )

    parser.add_argument(
        '--dataset-fraction',
        type=float,
        default=1.0,
        help='Fraction of dataset to use (0.0-1.0, default: 1.0)'
    )

    # MLflow arguments
    parser.add_argument(
        '--use-mlflow',
        action='store_true',
        help='Enable MLflow experiment tracking'
    )

    parser.add_argument(
        '--mlflow-experiment',
        type=str,
        default='esm2_contact_training',
        help='MLflow experiment name (default: esm2_contact_training)'
    )

    parser.add_argument(
        '--mlflow-tracking-uri',
        type=str,
        default='file:./mlruns',
        help='MLflow tracking URI (default: file:./mlruns)'
    )

    parser.add_argument(
        '--mlflow-run-name',
        type=str,
        default=None,
        help='MLflow run name (auto-generated if not specified)'
    )

    # Model architecture arguments
    parser.add_argument(
        '--in-channels',
        type=int,
        default=68,
        help='Number of input channels (default: 68 for 4 template + 64 ESM2)'
    )

    parser.add_argument(
        '--base-channels',
        type=int,
        default=32,
        help='Base number of channels in CNN (default: 32)'
    )

    parser.add_argument(
        '--dropout-rate',
        type=float,
        default=0.1,
        help='Dropout rate for regularization (default: 0.1)'
    )

    # Training arguments
    parser.add_argument(
        '--epochs',
        type=int,
        default=50,
        help='Maximum training epochs (default: 50)'
    )

    parser.add_argument(
        '--learning-rate',
        type=float,
        default=1e-3,
        help='Initial learning rate (default: 1e-3)'
    )

    parser.add_argument(
        '--weight-decay',
        type=float,
        default=1e-5,
        help='Weight decay for regularization (default: 1e-5)'
    )

    parser.add_argument(
        '--batch-size',
        type=int,
        default=4,
        help='Batch size (default: 4, use --adaptive-batching for automatic optimization)'
    )

    parser.add_argument(
        '--test-batch-size',
        type=int,
        default=None,
        help='Batch size for test evaluation (default: same as --batch-size)'
    )

    parser.add_argument(
        '--adaptive-batching',
        action='store_true',
        help='Enable automatic batch size optimization based on GPU memory'
    )

    parser.add_argument(
        '--max-batch-size',
        type=int,
        default=32,
        help='Maximum batch size for adaptive batching (default: 32)'
    )

    parser.add_argument(
        '--memory-utilization',
        type=float,
        default=0.7,
        help='Target GPU memory utilization for adaptive batching (0.0-1.0, default: 0.7)'
    )

    parser.add_argument(
        '--patience',
        type=int,
        default=10,
        help='Early stopping patience (default: 10)'
    )

    # Loss function arguments
    parser.add_argument(
        '--loss-type',
        type=str,
        choices=['bce', 'focal', 'weighted_bce'],
        default='bce',
        help='Loss function type (default: bce)'
    )

    parser.add_argument(
        '--pos-weight',
        type=float,
        default=5.0,
        help='Positive class weight for binary cross entropy (default: 5.0)'
    )

    # Optimization arguments
    parser.add_argument(
        '--use-amp',
        action='store_true',
        default=True,
        help='Use automatic mixed precision (default: enabled)'
    )

    parser.add_argument(
        '--no-amp',
        action='store_true',
        help='Disable automatic mixed precision'
    )

    parser.add_argument(
        '--memory-threshold',
        type=float,
        default=6.0,
        help='GPU memory threshold in GB (default: 6.0)'
    )

    parser.add_argument(
        '--progress-bar',
        action='store_true',
        default=True,
        help='Show progress bars during training (default: enabled)'
    )

    parser.add_argument(
        '--no-progress-bar',
        action='store_true',
        help='Disable progress bars'
    )

    # Output arguments
    parser.add_argument(
        '--output-dir',
        type=str,
        default='experiments',
        help='Output directory for experiments (default: experiments)'
    )

    parser.add_argument(
        '--verbose',
        action='store_true',
        default=False,
        help='Enable verbose output during training (default: enabled)'
    )

    parser.add_argument(
        '--quiet-progress',
        action='store_true',
        help='Reduce stdout noise during progress bar display (default: False)'
    )

    return parser.parse_args()


def validate_arguments(args: argparse.Namespace) -> None:
    """Validate command line arguments."""
    # Check dataset file exists
    if not Path(args.dataset_path).exists():
        raise FileNotFoundError(f"Dataset file not found: {args.dataset_path}")

    # Validate data split ratios
    total_ratio = args.train_ratio + args.val_ratio + args.test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        raise ValueError(f"Data split ratios must sum to 1.0, got {total_ratio:.4f}")

    if args.train_ratio <= 0 or args.val_ratio <= 0 or args.test_ratio <= 0:
        raise ValueError("All data split ratios must be positive")

    # Validate model parameters
    if args.in_channels <= 0:
        raise ValueError("in_channels must be positive")
    if args.base_channels <= 0:
        raise ValueError("base_channels must be positive")
    if not 0 <= args.dropout_rate < 1:
        raise ValueError("dropout_rate must be in [0, 1)")

    # Validate training parameters
    if args.epochs <= 0:
        raise ValueError("epochs must be positive")
    if args.learning_rate <= 0:
        raise ValueError("learning_rate must be positive")
    if args.batch_size <= 0:
        raise ValueError("batch_size must be positive")
    if args.test_batch_size is not None and args.test_batch_size <= 0:
        raise ValueError("test_batch_size must be positive")

    # Validate adaptive batching parameters
    if not 0.0 < args.memory_utilization <= 1.0:
        raise ValueError("memory_utilization must be in range (0.0, 1.0]")
    if args.max_batch_size <= 0:
        raise ValueError("max_batch_size must be positive integer")

    # Validate loss function parameter
    if args.pos_weight <= 0:
        raise ValueError("pos_weight must be positive")

    # Validate dataset fraction
    if not 0.0 < args.dataset_fraction <= 1.0:
        raise ValueError("dataset_fraction must be in range (0.0, 1.0]")


def create_dataset_subset(dataset, dataset_fraction: float, random_seed: int = 42):
    """Create a random subset of the dataset.

    Args:
        dataset: The original Tiny10Dataset
        dataset_fraction (float): Fraction of dataset to keep (0.0-1.0)
        random_seed (int): Random seed for reproducible sampling

    Returns:
        Subset of the original dataset
    """
    if dataset_fraction >= 1.0:
        return dataset

    total_size = len(dataset)
    subset_size = max(1, int(total_size * dataset_fraction))

    # Create reproducible random sampling
    generator = torch.Generator().manual_seed(random_seed)

    # Get random indices
    indices = torch.randperm(total_size, generator=generator)[:subset_size]

    # Create subset
    from torch.utils.data import Subset
    subset = Subset(dataset, indices.tolist())

    print(f"📊 Dataset subset created:")
    print(f"   Original size: {total_size} proteins")
    print(f"   Subset size: {len(subset)} proteins ({dataset_fraction*100:.1f}%)")

    return subset


def setup_experiment_directory(args: argparse.Namespace) -> Path:
    """Create experiment output directory."""
    experiment_dir = Path(args.output_dir) / args.experiment_name
    experiment_dir.mkdir(parents=True, exist_ok=True)
    return experiment_dir


def create_data_loaders(dataset, args: argparse.Namespace):
    """Create train, validation, and test data loaders."""
    # Create data splits
    train_dataset, val_dataset, test_dataset = create_data_splits(
        dataset,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        random_seed=args.random_seed
    )

    print(f"📊 Dataset splits created:")
    print(f"   Training proteins:   {len(train_dataset)} ({args.train_ratio*100:.0f}%)")
    print(f"   Validation proteins: {len(val_dataset)} ({args.val_ratio*100:.0f}%)")
    print(f"   Test proteins:       {len(test_dataset)} ({args.test_ratio*100:.0f}%)")

    # Create data loaders
    from torch.utils.data import DataLoader

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=True
    )

    # Use test batch size if specified, otherwise use training batch size
    test_batch_size = args.test_batch_size if args.test_batch_size is not None else args.batch_size

    test_loader = DataLoader(
        test_dataset,
        batch_size=test_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=True
    )

    return train_loader, val_loader, test_loader


def create_training_config(args: argparse.Namespace, dataset_path: str, experiment_dir: Path, test_batch_size: int) -> Dict[str, Any]:
    """Create training configuration dictionary."""
    # Handle AMP setting
    use_amp = args.use_amp and not args.no_amp

    # Handle progress bar setting
    use_progress_bar = args.progress_bar and not args.no_progress_bar

    config = {
        # Dataset configuration
        'dataset_path': dataset_path,
        'dataset_fraction': args.dataset_fraction,
        'train_ratio': args.train_ratio,
        'val_ratio': args.val_ratio,
        'test_ratio': args.test_ratio,
        'random_seed': args.random_seed,

        # Model configuration
        'in_channels': args.in_channels,
        'base_channels': args.base_channels,
        'dropout_rate': args.dropout_rate,

        # Training configuration
        'batch_size': args.batch_size,
        'test_batch_size': test_batch_size,
        'num_workers': 0,
        'num_epochs': args.epochs,
        'learning_rate': args.learning_rate,
        'weight_decay': args.weight_decay,
        'patience': args.patience,

        # Loss configuration
        'loss_type': args.loss_type,
        'pos_weight': args.pos_weight,

        # Optimization configuration
        'use_amp': use_amp,
        'memory_threshold': args.memory_threshold,

        # New: Adaptive batch configuration
        'adaptive_batching': args.adaptive_batching,
        'max_batch_size': args.max_batch_size,
        'memory_utilization': args.memory_utilization,

        # New: Progress bar configuration
        'use_progress_bar': use_progress_bar,

        # Experiment configuration
        'experiment_name': args.experiment_name,
        'experiment_dir': str(experiment_dir),
        'verbose': args.verbose,
        'quiet_progress': args.quiet_progress
    }

    return config


def save_experiment_results(experiment_dir: Path, history: Dict[str, np.ndarray],
                           best_auc: float, test_results: Dict[str, float],
                           model: torch.nn.Module, config: Dict[str, Any],
                           training_time: float) -> None:
    """Save all experiment results and artifacts."""
    # Save training history
    history_file = experiment_dir / 'training_history.npy'
    np.save(history_file, history)

    # Save final results
    results = {
        'experiment_name': config['experiment_name'],
        'best_val_auc': float(best_auc),
        'test_results': {k: float(v) for k, v in test_results.items()},
        'training_time_minutes': training_time / 60,
        'total_epochs': len(history['train_loss']),
        'config': config,
        'model_info': model.get_model_info()
    }

    results_file = experiment_dir / 'results.json'
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

    # Save trained model
    model_file = experiment_dir / 'model.pth'
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': model.get_model_info(),
        'best_auc': best_auc,
        'test_results': test_results,
        'config': config
    }, model_file)

    # Save configuration separately for easy reference
    config_file = experiment_dir / 'config.json'
    with open(config_file, 'w') as f:
        json.dump(config, f, indent=2)

    print(f"💾 Results saved to {experiment_dir}:")
    print(f"   📄 Results: {results_file}")
    print(f"   📈 History: {history_file}")
    print(f"   🧠 Model: {model_file}")
    print(f"   ⚙️  Config: {config_file}")


def main():
    """Main training pipeline."""
    print("🚀 CNN Training for Protein Contact Prediction")
    print("=" * 60)
    print("Lightweight wrapper leveraging src/esm2_contact/training/ framework")
    print()

    # Parse and validate arguments
    args = parse_arguments()
    validate_arguments(args)

    # Setup experiment directory
    experiment_dir = setup_experiment_directory(args)
    print(f"📁 Experiment directory: {experiment_dir}")

    # Print configuration
    print(f"📋 Training Configuration:")
    print(f"   Dataset: {args.dataset_path}")
    print(f"   Experiment: {args.experiment_name}")
    if args.dataset_fraction < 1.0:
        print(f"   Dataset fraction: {args.dataset_fraction*100:.1f}%")
    print(f"   Data split: {args.train_ratio:.1f}/{args.val_ratio:.1f}/{args.test_ratio:.1f}")
    print(f"   Model: {args.in_channels}→{args.base_channels} channels")
    print(f"   Training: {args.epochs} epochs, LR={args.learning_rate}")
    print(f"   Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    print()

    # Load dataset
    print("📊 Loading dataset...")
    full_dataset = Tiny10Dataset(args.dataset_path, validate_data=True, verbose=True)

    # Create dataset subset if requested
    if args.dataset_fraction < 1.0:
        dataset = create_dataset_subset(full_dataset, args.dataset_fraction, args.random_seed)
    else:
        dataset = full_dataset

    # Create data loaders
    train_loader, val_loader, test_loader = create_data_loaders(dataset, args)

    # Create model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = BinaryContactCNN(
        in_channels=args.in_channels,
        base_channels=args.base_channels,
        dropout_rate=args.dropout_rate
    )
    model = model.to(device)

    print(f"🧠 Model created: {model.get_model_info()['total_parameters']:,} parameters")

    # Calculate test batch size for config
    test_batch_size = args.test_batch_size if args.test_batch_size is not None else args.batch_size

    # Create training configuration
    config = create_training_config(args, args.dataset_path, experiment_dir, test_batch_size)

    # Setup MLflow if enabled
    if args.use_mlflow:
        try:
            from src.esm2_contact.mlflow_utils import setup_mlflow, enable_pytorch_autolog

            # Setup MLflow tracking
            setup_mlflow(
                tracking_uri=args.mlflow_tracking_uri,
                experiment_name=args.mlflow_experiment
            )

            # Enable PyTorch autologging
            enable_pytorch_autolog(log_models=False, exclusive=True)  # We'll log models manually

            print("✅ MLflow tracking enabled")
        except ImportError:
            print("⚠️  MLflow not available. Install mlflow package to enable tracking.")
            args.use_mlflow = False

    # Create trainer and start training
    # Default to verbose=True for better user experience (progress bars, etc.)
    effective_verbose = not args.quiet_progress  # Enable unless explicitly quiet
    trainer = CNNTrainer(
        model,
        device,
        verbose=effective_verbose,
        enable_mlflow=args.use_mlflow,
        mlflow_experiment=args.mlflow_experiment
    )

    print(f"\n🎯 Starting training...")
    start_time = time.time()

    try:
        # Train model
        history, best_auc = trainer.train(train_loader, val_loader, config)

        # Evaluate on test set
        test_results = trainer.evaluate(test_loader, config)

        training_time = time.time() - start_time

        # Save results
        save_experiment_results(
            experiment_dir, history, best_auc, test_results,
            model, config, training_time
        )

        # Print final results
        print(f"\n🎉 Training completed successfully!")
        print(f"   🎯 Best validation AUC: {best_auc:.4f}")
        print(f"   🧪 Test AUC: {test_results.get('test_auc', 'N/A'):.4f}")
        print(f"   ⏱️  Training time: {training_time/60:.1f} minutes")

        # Performance assessment
        if best_auc > 0.8:
            performance = "🌟 Excellent"
        elif best_auc > 0.7:
            performance = "✅ Good"
        elif best_auc > 0.6:
            performance = "⚠️  Moderate"
        else:
            performance = "❌ Needs Improvement"

        print(f"   🏆 Performance Assessment: {performance}")

        if best_auc > 0.8:
            print(f"   🌟 ACHIEVED EXCELLENT RESULTS: {best_auc:.4f} (> 0.8)")

        return 0

    except KeyboardInterrupt:
        print(f"\n⚠️  Training interrupted by user")
        return 1
    except Exception as e:
        print(f"\n❌ Training failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    warnings.filterwarnings('ignore')
    sys.exit(main())